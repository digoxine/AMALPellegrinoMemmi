{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from pathlib import Path\n",
    "import ipyparams\n",
    "\n",
    "\n",
    "import datetime, os\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 7200 (pid 8924), started 21:44:19 ago. (Use '!kill 8924' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-80acf7c5f3ef6494\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-80acf7c5f3ef6494\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 7200;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logs_base_dir = \"./runs/\"\n",
    "os.makedirs(logs_base_dir, exist_ok=True)\n",
    "%tensorboard --logdir {logs_base_dir} --port=7200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce TME, contrairement aux autres TMES précédents, nous utilisons un embedding plutôt qu'un One Hot Encoding. Chose qui n'était pas intéressante dans les TMES précédents. Nous recherchons ainsi des ressemblances au sein des caractères des séquences sur lesquelles nous apprenons. Chose qui augmente l'efficacité comparé au simple One Hot Encoding du TME 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons utilisé 40 itérations pour chaque type de réseaux, un batch de taille 64 ainsi qu'un embedding size de 97 (la taille du dictionnaire de lettres). À chaque itération, nous avons généré une phrase à partir de la séquence 'He is '.\n",
    "Nous constatons que plus les itérations avancent et plus les phrases ont un sens.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sans suprise, nous constatons que le LSTM est plus efficace pour générer une phrase correcte. Ce qui est logique, dans la mesure où le LSTM est un GRU avec en plus une mémoire. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En revanche nous pouvons voir que les temps d'exécutions sont relativement importants dans nos implémentations. Il serait intéressant de voir à quel point la bibliothèque Pytorch propose des implémentations efficaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
